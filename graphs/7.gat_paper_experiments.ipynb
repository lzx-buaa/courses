{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lasting-portsmouth",
   "metadata": {},
   "source": [
    "# Paper: \n",
    "**Title**:  \n",
    "   - Graph Attention Networks  \n",
    "\n",
    "**Authors**:  \n",
    "   - Petar Velickovic et al\n",
    "    \n",
    "**Published at**:  \n",
    "   - ICLR 2018  \n",
    "   \n",
    "**Highlights**:  \n",
    "   - Readily applicable in inductive and transductive settings\n",
    "   - Division between spectral approaches (eigendecomposition of graph laplacian) and non-spectral approaches\n",
    "       - Spectral methods $\\rightarrow$ depends on laplacian eigenbasis $\\rightarrow$ known graph structure\n",
    "   - Benefit of using attention:\n",
    "       - Variable size input\n",
    "       - Compute hidden representations of each node, but attention over their neighbors (_**self attention**_)\n",
    "   - Input $\\rightarrow$ Node Features $ h = \\{\\overrightarrow{h_{1}}, \\overrightarrow{h_{2}}, ..., \\overrightarrow{h_{n}} \\}, h_{i}$ \n",
    "   - Model allows every node attend to its neighbor, dropping all structural information\n",
    "   - Inject structure by masked attention\n",
    "   \n",
    "$$\n",
    "\\alpha_{i,j} = \\frac{\\exp({LeakyRelu(\\overrightarrow{a^T [W h_i || W h_j]}}))}{\\sum_{k\\in N_{i}}\\exp({LeakyRelu(\\overrightarrow{a^T [W h_i || W h_k]}}))}\n",
    "$$\n",
    "\n",
    "   \n",
    "$$\n",
    "\\overrightarrow{h'_i} = \\sigma(\\sum_{j\\in N_{i}}\\alpha_{i,j}W \\overrightarrow{h_j})\n",
    "$$\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-atlanta",
   "metadata": {},
   "source": [
    "<img src=\"GCN_vs_GAT.jpeg\" width=\"640\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "differential-concentration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.label_propagation.ipynb        requirements.txt\r\n",
      "2.embeddings_benchmark_GEM.ipynb sbm.gpickle\r\n",
      "3.graph_jazz_embeddings.ipynb    sbm_node_labels.pickle\r\n",
      "4.spectral_clustering.ipynb      water_drop.gif\r\n",
      "5.gcn_message_passing.ipynb      water_drop.mp4\r\n",
      "6.gcn_paper.ipynb                water_drop_2.mp4\r\n",
      "7.gat_paper_experiments.ipynb    water_drop_3.mp4\r\n",
      "GCN_vs_GAT.jpeg                  water_drop_4.mp4\r\n",
      "karate.edgelist\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-conference",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "refined-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-lighting",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "twelve-hardwood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2]) 10\n"
     ]
    }
   ],
   "source": [
    "in_features = 5\n",
    "out_features = 2\n",
    "nb_nodes = 10\n",
    "\n",
    "W = nn.Parameter(torch.zeros(size=(in_features, out_features))) #xavier paramiter inizializator\n",
    "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
    "\n",
    "x = torch.rand(nb_nodes,in_features) \n",
    "h = torch.mm(x, W)\n",
    "N = h.size()[0]\n",
    "\n",
    "print(h.shape, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "moral-mortality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "α = nn.Parameter(torch.zeros(size=(2*out_features, 1))) \n",
    "nn.init.xavier_uniform_(α.data, gain=1.414)\n",
    "print(α.shape)\n",
    "\n",
    "leakyrelu = nn.LeakyReLU(0.2)  # LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "heavy-russia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0294,  0.1760,  0.0294,  0.1760],\n",
       "         [ 0.0294,  0.1760, -1.3731,  0.0782],\n",
       "         [ 0.0294,  0.1760, -0.1718,  0.1770],\n",
       "         [ 0.0294,  0.1760, -1.2101,  1.0527],\n",
       "         [ 0.0294,  0.1760, -0.8503, -0.1972],\n",
       "         [ 0.0294,  0.1760, -0.5405,  0.0592],\n",
       "         [ 0.0294,  0.1760, -0.8650,  0.1249],\n",
       "         [ 0.0294,  0.1760, -0.5202,  0.7897],\n",
       "         [ 0.0294,  0.1760, -0.1016, -0.0373],\n",
       "         [ 0.0294,  0.1760, -0.1864, -0.5183]],\n",
       "\n",
       "        [[-1.3731,  0.0782,  0.0294,  0.1760],\n",
       "         [-1.3731,  0.0782, -1.3731,  0.0782],\n",
       "         [-1.3731,  0.0782, -0.1718,  0.1770],\n",
       "         [-1.3731,  0.0782, -1.2101,  1.0527],\n",
       "         [-1.3731,  0.0782, -0.8503, -0.1972],\n",
       "         [-1.3731,  0.0782, -0.5405,  0.0592],\n",
       "         [-1.3731,  0.0782, -0.8650,  0.1249],\n",
       "         [-1.3731,  0.0782, -0.5202,  0.7897],\n",
       "         [-1.3731,  0.0782, -0.1016, -0.0373],\n",
       "         [-1.3731,  0.0782, -0.1864, -0.5183]],\n",
       "\n",
       "        [[-0.1718,  0.1770,  0.0294,  0.1760],\n",
       "         [-0.1718,  0.1770, -1.3731,  0.0782],\n",
       "         [-0.1718,  0.1770, -0.1718,  0.1770],\n",
       "         [-0.1718,  0.1770, -1.2101,  1.0527],\n",
       "         [-0.1718,  0.1770, -0.8503, -0.1972],\n",
       "         [-0.1718,  0.1770, -0.5405,  0.0592],\n",
       "         [-0.1718,  0.1770, -0.8650,  0.1249],\n",
       "         [-0.1718,  0.1770, -0.5202,  0.7897],\n",
       "         [-0.1718,  0.1770, -0.1016, -0.0373],\n",
       "         [-0.1718,  0.1770, -0.1864, -0.5183]],\n",
       "\n",
       "        [[-1.2101,  1.0527,  0.0294,  0.1760],\n",
       "         [-1.2101,  1.0527, -1.3731,  0.0782],\n",
       "         [-1.2101,  1.0527, -0.1718,  0.1770],\n",
       "         [-1.2101,  1.0527, -1.2101,  1.0527],\n",
       "         [-1.2101,  1.0527, -0.8503, -0.1972],\n",
       "         [-1.2101,  1.0527, -0.5405,  0.0592],\n",
       "         [-1.2101,  1.0527, -0.8650,  0.1249],\n",
       "         [-1.2101,  1.0527, -0.5202,  0.7897],\n",
       "         [-1.2101,  1.0527, -0.1016, -0.0373],\n",
       "         [-1.2101,  1.0527, -0.1864, -0.5183]],\n",
       "\n",
       "        [[-0.8503, -0.1972,  0.0294,  0.1760],\n",
       "         [-0.8503, -0.1972, -1.3731,  0.0782],\n",
       "         [-0.8503, -0.1972, -0.1718,  0.1770],\n",
       "         [-0.8503, -0.1972, -1.2101,  1.0527],\n",
       "         [-0.8503, -0.1972, -0.8503, -0.1972],\n",
       "         [-0.8503, -0.1972, -0.5405,  0.0592],\n",
       "         [-0.8503, -0.1972, -0.8650,  0.1249],\n",
       "         [-0.8503, -0.1972, -0.5202,  0.7897],\n",
       "         [-0.8503, -0.1972, -0.1016, -0.0373],\n",
       "         [-0.8503, -0.1972, -0.1864, -0.5183]],\n",
       "\n",
       "        [[-0.5405,  0.0592,  0.0294,  0.1760],\n",
       "         [-0.5405,  0.0592, -1.3731,  0.0782],\n",
       "         [-0.5405,  0.0592, -0.1718,  0.1770],\n",
       "         [-0.5405,  0.0592, -1.2101,  1.0527],\n",
       "         [-0.5405,  0.0592, -0.8503, -0.1972],\n",
       "         [-0.5405,  0.0592, -0.5405,  0.0592],\n",
       "         [-0.5405,  0.0592, -0.8650,  0.1249],\n",
       "         [-0.5405,  0.0592, -0.5202,  0.7897],\n",
       "         [-0.5405,  0.0592, -0.1016, -0.0373],\n",
       "         [-0.5405,  0.0592, -0.1864, -0.5183]],\n",
       "\n",
       "        [[-0.8650,  0.1249,  0.0294,  0.1760],\n",
       "         [-0.8650,  0.1249, -1.3731,  0.0782],\n",
       "         [-0.8650,  0.1249, -0.1718,  0.1770],\n",
       "         [-0.8650,  0.1249, -1.2101,  1.0527],\n",
       "         [-0.8650,  0.1249, -0.8503, -0.1972],\n",
       "         [-0.8650,  0.1249, -0.5405,  0.0592],\n",
       "         [-0.8650,  0.1249, -0.8650,  0.1249],\n",
       "         [-0.8650,  0.1249, -0.5202,  0.7897],\n",
       "         [-0.8650,  0.1249, -0.1016, -0.0373],\n",
       "         [-0.8650,  0.1249, -0.1864, -0.5183]],\n",
       "\n",
       "        [[-0.5202,  0.7897,  0.0294,  0.1760],\n",
       "         [-0.5202,  0.7897, -1.3731,  0.0782],\n",
       "         [-0.5202,  0.7897, -0.1718,  0.1770],\n",
       "         [-0.5202,  0.7897, -1.2101,  1.0527],\n",
       "         [-0.5202,  0.7897, -0.8503, -0.1972],\n",
       "         [-0.5202,  0.7897, -0.5405,  0.0592],\n",
       "         [-0.5202,  0.7897, -0.8650,  0.1249],\n",
       "         [-0.5202,  0.7897, -0.5202,  0.7897],\n",
       "         [-0.5202,  0.7897, -0.1016, -0.0373],\n",
       "         [-0.5202,  0.7897, -0.1864, -0.5183]],\n",
       "\n",
       "        [[-0.1016, -0.0373,  0.0294,  0.1760],\n",
       "         [-0.1016, -0.0373, -1.3731,  0.0782],\n",
       "         [-0.1016, -0.0373, -0.1718,  0.1770],\n",
       "         [-0.1016, -0.0373, -1.2101,  1.0527],\n",
       "         [-0.1016, -0.0373, -0.8503, -0.1972],\n",
       "         [-0.1016, -0.0373, -0.5405,  0.0592],\n",
       "         [-0.1016, -0.0373, -0.8650,  0.1249],\n",
       "         [-0.1016, -0.0373, -0.5202,  0.7897],\n",
       "         [-0.1016, -0.0373, -0.1016, -0.0373],\n",
       "         [-0.1016, -0.0373, -0.1864, -0.5183]],\n",
       "\n",
       "        [[-0.1864, -0.5183,  0.0294,  0.1760],\n",
       "         [-0.1864, -0.5183, -1.3731,  0.0782],\n",
       "         [-0.1864, -0.5183, -0.1718,  0.1770],\n",
       "         [-0.1864, -0.5183, -1.2101,  1.0527],\n",
       "         [-0.1864, -0.5183, -0.8503, -0.1972],\n",
       "         [-0.1864, -0.5183, -0.5405,  0.0592],\n",
       "         [-0.1864, -0.5183, -0.8650,  0.1249],\n",
       "         [-0.1864, -0.5183, -0.5202,  0.7897],\n",
       "         [-0.1864, -0.5183, -0.1016, -0.0373],\n",
       "         [-0.1864, -0.5183, -0.1864, -0.5183]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_features)\n",
    "a_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "minute-polyester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0987, -0.0355,  0.0499, -0.1462,  0.1152,  0.0334, -0.0168, -0.0812,\n",
       "          0.1970,  0.4693],\n",
       "        [ 0.9173,  0.6413,  0.8686,  0.0876,  0.9338,  0.8520,  0.7344,  0.4126,\n",
       "          1.0157,  1.2879],\n",
       "        [ 0.2347, -0.0082,  0.1860, -0.1190,  0.2513,  0.1695,  0.0519, -0.0540,\n",
       "          0.3331,  0.6054],\n",
       "        [ 2.0172,  1.7412,  1.9685,  1.1875,  2.0337,  1.9519,  1.8343,  1.5125,\n",
       "          2.1156,  2.3878],\n",
       "        [ 0.2252, -0.0102,  0.1765, -0.1209,  0.2418,  0.1599,  0.0423, -0.0559,\n",
       "          0.3236,  0.5959],\n",
       "        [ 0.3357,  0.0597,  0.2870, -0.0988,  0.3523,  0.2704,  0.1528, -0.0338,\n",
       "          0.4341,  0.7063],\n",
       "        [ 0.6348,  0.3588,  0.5860, -0.0390,  0.6513,  0.5695,  0.4519,  0.1301,\n",
       "          0.7331,  1.0054],\n",
       "        [ 1.2284,  0.9524,  1.1797,  0.3987,  1.2449,  1.1631,  1.0455,  0.7237,\n",
       "          1.3268,  1.5990],\n",
       "        [-0.0156, -0.0708, -0.0254, -0.1816, -0.0123, -0.0287, -0.0522, -0.1166,\n",
       "          0.0202,  0.2925],\n",
       "        [-0.1236, -0.1788, -0.1334, -0.2896, -0.1203, -0.1367, -0.1602, -0.2246,\n",
       "         -0.1040, -0.0495]], grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = leakyrelu(torch.matmul(a_input, α).squeeze(2))\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "secure-disney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0499, -0.1462,  0.1152,  0.0000, -0.0168, -0.0812,\n",
       "          0.1970,  0.0000],\n",
       "        [ 0.0000,  0.6413,  0.0000,  0.0876,  0.9338,  0.0000,  0.7344,  0.4126,\n",
       "          1.0157,  0.0000],\n",
       "        [ 0.2347, -0.0082,  0.0000,  0.0000,  0.2513,  0.1695,  0.0000,  0.0000,\n",
       "          0.3331,  0.0000],\n",
       "        [ 2.0172,  1.7412,  1.9685,  1.1875,  0.0000,  1.9519,  0.0000,  1.5125,\n",
       "          2.1156,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2418,  0.0000,  0.0423,  0.0000,\n",
       "          0.3236,  0.0000],\n",
       "        [ 0.3357,  0.0597,  0.2870, -0.0988,  0.3523,  0.2704,  0.0000, -0.0338,\n",
       "          0.4341,  0.7063],\n",
       "        [ 0.6348,  0.0000,  0.0000, -0.0390,  0.6513,  0.5695,  0.0000,  0.1301,\n",
       "          0.0000,  1.0054],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.3987,  1.2449,  1.1631,  1.0455,  0.7237,\n",
       "          1.3268,  1.5990],\n",
       "        [ 0.0000,  0.0000, -0.0254, -0.1816, -0.0123, -0.0287, -0.0522, -0.1166,\n",
       "          0.0202,  0.2925],\n",
       "        [-0.1236, -0.1788,  0.0000,  0.0000,  0.0000, -0.1367, -0.1602,  0.0000,\n",
       "          0.0000,  0.0000]], grad_fn=<SWhereBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Masked Attention\n",
    "adj = torch.randint(2, (nb_nodes, nb_nodes))\n",
    "attention = torch.where(adj > 0, e, torch.zeros_like(e))\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "mineral-regulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1001, 0.0966, 0.1001, 0.0865, 0.1001, 0.1035, 0.0985, 0.0923, 0.1219,\n",
       "         0.1001],\n",
       "        [0.0632, 0.0632, 0.0632, 0.0690, 0.1608, 0.1481, 0.1317, 0.0632, 0.1745,\n",
       "         0.0632],\n",
       "        [0.0964, 0.0964, 0.1161, 0.0855, 0.1239, 0.0964, 0.1015, 0.0913, 0.0964,\n",
       "         0.0964],\n",
       "        [0.0224, 0.0224, 0.1604, 0.0224, 0.0224, 0.1577, 0.1402, 0.0224, 0.1858,\n",
       "         0.2439],\n",
       "        [0.1186, 0.0947, 0.0947, 0.0839, 0.0947, 0.0947, 0.0988, 0.0947, 0.1308,\n",
       "         0.0947],\n",
       "        [0.0885, 0.0939, 0.0885, 0.0801, 0.0885, 0.1159, 0.0885, 0.0885, 0.0885,\n",
       "         0.1793],\n",
       "        [0.1446, 0.1097, 0.0766, 0.0766, 0.0766, 0.0766, 0.0766, 0.0766, 0.0766,\n",
       "         0.2094],\n",
       "        [0.0408, 0.1056, 0.0408, 0.0607, 0.1415, 0.1304, 0.0408, 0.0841, 0.1536,\n",
       "         0.2017],\n",
       "        [0.1000, 0.1000, 0.0975, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1021,\n",
       "         0.1000],\n",
       "        [0.1051, 0.1051, 0.1051, 0.1051, 0.0932, 0.0917, 0.0896, 0.1051, 0.0947,\n",
       "         0.1051]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = F.softmax(attention, dim=1)\n",
    "h_prime  = torch.matmul(attention, h)\n",
    "print(attention.shape)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "conservative-grounds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12ed2a438>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(attention.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-vatican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-interim",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.dropout       = dropout        # drop prob = 0.6\n",
    "        self.in_features   = in_features    # \n",
    "        self.out_features  = out_features   # \n",
    "        self.alpha         = alpha          # LeakyReLU with negative input slope, alpha = 0.2\n",
    "        self.concat        = concat         # conacat = True for all layers except the output layer.\n",
    "\n",
    "        \n",
    "        # Xavier Initialization of Weights\n",
    "        # Alternatively use weights_init to apply weights of choice \n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        \n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        \n",
    "        # LeakyReLU\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # Linear Transformation\n",
    "        h = torch.mm(input, self.W) # matrix multiplication\n",
    "        N = h.size()[0]\n",
    "        print(N)\n",
    "\n",
    "        # Attention Mechanism\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e       = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        # Masked Attention\n",
    "        zero_vec  = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        \n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime   = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "name_data = 'Cora'\n",
    "dataset = Planetoid(root= '/tmp/' + name_data, name = name_data)\n",
    "dataset.transform = T.NormalizeFeatures()\n",
    "\n",
    "print(f\"Number of Classes in {name_data}:\", dataset.num_classes)\n",
    "print(f\"Number of Node Features in {name_data}:\", dataset.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid = 8\n",
    "        self.in_head = 8\n",
    "        self.out_head = 1\n",
    "        \n",
    "        \n",
    "        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
    "        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False,\n",
    "                             heads=self.out_head, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "                \n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT().to(device)\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    if epoch%200 == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-microwave",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "1. [Understanding Graph Attention Networks (GAT)](https://dsgiitr.com/blogs/gat/)\n",
    "2. [GAT Implementation in PyTorch Geometric](https://github.com/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial3/Tutorial3.ipynb)  \n",
    "3. [PyTorch Geometric tutorial](https://antoniolonga.github.io/Pytorch_geometric_tutorials/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-piano",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
