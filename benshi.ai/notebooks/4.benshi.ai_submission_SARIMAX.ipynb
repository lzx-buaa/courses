{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = pd.read_csv(\"../data_forecasting/Train.csv\")\n",
    "train_instances.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances['timestamp'] = train_instances['year'].astype('str') + \"-\" + train_instances['month'].astype('str')\n",
    "train_instances.drop(columns={'region', \n",
    "                      'district', \n",
    "                      'stock_initial', \n",
    "                      'stock_received', \n",
    "                      'stock_adjustment',\n",
    "                      'stock_end',\n",
    "                      'average_monthly_consumption',\n",
    "                      'stock_stockout_days',\n",
    "                      'stock_ordered'},\n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances['timestamp']=pd.to_datetime(train_instances['timestamp'])\n",
    "train_instances.index= train_instances.timestamp\n",
    "train_instances.drop(columns={'timestamp'},inplace=True)\n",
    "\n",
    "## Sort Dataframe by Date - Causality in Preds\n",
    "train_instances.sort_values(by='timestamp', inplace=True)\n",
    "train_instances.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances['stock_distributed'][train_instances.product_code=='AS27137'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances['stock_distributed'][train_instances.product_code=='AS27137']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites = train_instances.site_code.unique()\n",
    "len(all_sites)\n",
    "all_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products =list(train_instances.product_code.unique())\n",
    "print(len(all_products))\n",
    "all_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-orlando",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_encoder = LabelEncoder()\n",
    "product_encoder = LabelEncoder()\n",
    "#train_instances.drop(columns=['timestamp'], inplace=True)\n",
    "train_instances['site_code_encoded'] = site_encoder.fit_transform(train_instances['site_code'])\n",
    "train_instances['product_code_encoded'] = product_encoder.fit_transform(train_instances['product_code'])\n",
    "train_instances.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances['site_code_encoded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances['product_code_encoded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances['stock_distributed'][train_instances.product_code=='AS27137']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "train_instances['stock_distributed'][train_instances.product_code=='AS27137'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_encoder.transform(['AS27137'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "train_instances['stock_distributed'][train_instances.product_code_encoded==6].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset=train_instances\n",
    "#final_dataset=train_instances[['site_code_encoded', 'product_code_encoded', 'stock_distributed']]\n",
    "final_dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-albania",
   "metadata": {},
   "source": [
    "## StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train = final_dataset[:int(train_size*(len(final_dataset)))]\n",
    "valid = final_dataset[int(train_size*(len(final_dataset))):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['site_code_encoded','product_code_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.statespace.SARIMAX(endog=train['stock_distributed'],\n",
    "                                  exog=train[['site_code_encoded','product_code_encoded']],\n",
    "                                  order=(2,0,0),\n",
    "                                  #order=(1,1,0),\n",
    "                                  #order=(7,1,7),\n",
    "                                  enforce_stationarity=True)\n",
    "sarima = model.fit()\n",
    "print(sarima.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sarima.forecast(steps=len(valid), exog=valid[['site_code_encoded','product_code_encoded']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=preds.reset_index(drop=True)\n",
    "preds.index=valid.index\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_preds = pd.concat([valid, preds], axis=1)\n",
    "full_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = full_preds['stock_distributed'][full_preds.product_code_encoded==6]\n",
    "tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=full_preds['predicted_mean'][full_preds.product_code_encoded==6]\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_label = 7\n",
    "pd.DataFrame({'test':full_preds['stock_distributed'][full_preds.product_code_encoded==prod_label],\n",
    "              'pred':full_preds['predicted_mean'][full_preds.product_code_encoded==prod_label]}).plot();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-hundred",
   "metadata": {},
   "source": [
    "### Filtering by Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_label=6\n",
    "train_filtered = train[train.product_code_encoded==product_label]\n",
    "valid_filtered = valid[valid.product_code_encoded==product_label]\n",
    "train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(train_filtered.index, train_filtered['stock_distributed'], width=10)\n",
    "ax.xaxis_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = sm.tsa.statespace.SARIMAX(endog=train_filtered['stock_distributed'],\n",
    "                                  exog=train_filtered[['site_code_encoded']],\n",
    "                                  order=(1,0,0))\n",
    "sarima_2 = model_2.fit()\n",
    "print(sarima_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_filtered[['site_code_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_filtered = sarima_2.forecast(steps=len(valid_filtered), exog=valid_filtered[['site_code_encoded']])\n",
    "preds_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_filtered=preds_filtered.reset_index(drop=True)\n",
    "preds_filtered.index=valid_filtered.index\n",
    "full_preds_filtered = pd.concat([valid_filtered, preds_filtered], axis=1)\n",
    "full_preds_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(full_preds_filtered.index, full_preds_filtered['stock_distributed'], width=10)\n",
    "ax.bar(full_preds_filtered.index, full_preds_filtered['predicted_mean'], width=10)\n",
    "ax.xaxis_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-atlanta",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def onehot_encode(df, onehot_columns):\n",
    "    ct = ColumnTransformer(\n",
    "        [('onehot', OneHotEncoder(drop='first'), onehot_columns)],\n",
    "        remainder='passthrough'\n",
    "        )\n",
    "    return ct.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode_pd(df, col_name):\n",
    "    dummies = pd.get_dummies(df[col_name], prefix=col_name)\n",
    "    return pd.concat([df, dummies], axis=1).drop(columns=[col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(train['site_code'], prefix='site_code')\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = pd.concat([train, dummies], axis=1)\n",
    "new_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-picture",
   "metadata": {},
   "source": [
    "## Other Dataset Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_dataset[['year','month','site_code_encoded','product_code_encoded','stock_distributed']].reset_index(drop=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train = final_df[:int(train_size*(len(final_df)))]\n",
    "valid = final_df[int(train_size*(len(final_df))):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_arr = scaler.fit_transform(train[['year','month','site_code_encoded','product_code_encoded']])\n",
    "X_val_arr = scaler.transform(valid[['year','month','site_code_encoded','product_code_encoded']])\n",
    "\n",
    "y_train_arr = scaler.fit_transform(train[['stock_distributed']])\n",
    "y_val_arr = scaler.transform(valid[['stock_distributed']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device}\" \" is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_features = torch.Tensor(X_train_arr)\n",
    "train_targets = torch.Tensor(y_train_arr)\n",
    "val_features = torch.Tensor(X_val_arr)\n",
    "val_targets = torch.Tensor(y_val_arr)\n",
    "\n",
    "train = TensorDataset(train_features, train_targets)\n",
    "val = TensorDataset(val_features, val_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, h0 = self.rnn(x, h0.detach())\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, seq_len, n_layers=2):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.seq_len = seq_len\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "          input_size=n_features,\n",
    "          hidden_size=n_hidden,\n",
    "          num_layers=n_layers,\n",
    "          dropout=0.5\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=n_hidden, out_features=1)\n",
    "        self.reset_hidden_state()\n",
    "        \n",
    "    def reset_hidden_state(self):\n",
    "        self.hidden = (\n",
    "            torch.zeros(self.n_layers, self.seq_len, self.n_hidden),\n",
    "            torch.zeros(self.n_layers, self.seq_len, self.n_hidden)\n",
    "        )\n",
    "    def forward(self, sequences):\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "          sequences.view(len(sequences), self.seq_len, -1),\n",
    "          self.hidden\n",
    "        )\n",
    "        last_time_step = \\\n",
    "          lstm_out.view(self.seq_len, len(sequences), self.n_hidden)[-1]\n",
    "        y_pred = self.linear(last_time_step)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_arr.shape[1]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_arr.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dim = 64\n",
    "layer_dim = 3\n",
    "batch_size = 64\n",
    "dropout = 0.2\n",
    "n_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_params = {'input_dim': input_dim,\n",
    "#                 'hidden_dim' : hidden_dim,\n",
    "#                 'layer_dim' : layer_dim,\n",
    "#                 'output_dim' : output_dim,\n",
    "#                 'dropout_prob' : dropout}\n",
    "\n",
    "model_params = {\n",
    "    'n_features' : input_dim,\n",
    "    'n_hidden': hidden_dim,\n",
    "    'seq_len': output_dim,\n",
    "    'n_layers': layer_dim\n",
    "}\n",
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMPredictor(**model_params)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epocs = 2\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "residential-stationery",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-908eb5b3e66d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f'Epoch {epoch}')\n",
    "    for step, (x,y) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        model.reset_hidden_state()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step%100 == 0:\n",
    "            for step_valid, (x_valid,y_valid) in val_loader:\n",
    "                with torch.no_grad():\n",
    "                    output_valid = model.forward(x_valid)\n",
    "                mse = mean_squared_error(y_valid, output_valid)\n",
    "        print(f\"Iteration: {step}; Loss: {loss.item()}; MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-shark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-beaver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(num_epochs):\n",
    "    for x,y in \n",
    "    model.reset_hidden_state()\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred.float(), y_train)\n",
    "    if test_data is not None:\n",
    "      with torch.no_grad():\n",
    "        y_test_pred = model(X_test)\n",
    "        test_loss = loss_fn(y_test_pred.float(), y_test)\n",
    "      test_hist[t] = test_loss.item()\n",
    "      if t % 10 == 0:\n",
    "        print(f'Epoch {t} train loss: {loss.item()} test loss: {test_loss.item()}')\n",
    "    elif t % 10 == 0:\n",
    "      print(f'Epoch {t} train loss: {loss.item()}')\n",
    "    train_hist[t] = loss.item()\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "      model,\n",
    "      train_data,\n",
    "      train_labels,\n",
    "      test_data=None,\n",
    "      test_labels=None\n",
    "):\n",
    "  loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "  optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  num_epochs = 60\n",
    "  train_hist = np.zeros(num_epochs)\n",
    "  test_hist = np.zeros(num_epochs)\n",
    "  for t in range(num_epochs):\n",
    "    model.reset_hidden_state()\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred.float(), y_train)\n",
    "    if test_data is not None:\n",
    "      with torch.no_grad():\n",
    "        y_test_pred = model(X_test)\n",
    "        test_loss = loss_fn(y_test_pred.float(), y_test)\n",
    "      test_hist[t] = test_loss.item()\n",
    "      if t % 10 == 0:\n",
    "        print(f'Epoch {t} train loss: {loss.item()} test loss: {test_loss.item()}')\n",
    "    elif t % 10 == 0:\n",
    "      print(f'Epoch {t} train loss: {loss.item()}')\n",
    "    train_hist[t] = loss.item()\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "  return model.eval(), train_hist, test_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-stupid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "curious-pavilion",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(full_preds['stock_distributed'], full_preds['predicted_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-bicycle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sixth-victim",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "https://www.kaggle.com/poiupoiu/how-to-use-sarimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-capitol",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
